# Anytime Stereo Image Depth Estimation on Mobile Devices 论文阅读



# 简介
在移动设备比如移动机器人，无人汽车上双目深度估计通常被期望能够在有限的计算资源下实时的估计出精确的视差图。现在的许多基于深度学习的双目深度估计的工作都需要在计算结果精度与整体效率之间做出权衡，而且网络参数很多。面对这些短板，AnyNet 可以在推理时权衡效率和准确性。深度估计是分阶段进行的，在此期间可以随时查询模型以输出其当前的最佳估计。
<br/>
<br/>
<br/>
<br/>

# 介绍
使用CNN来预测视差图时的计算复杂度通常和图像的分辨率成三次方，以及与所考虑的最大视差成线性关系。所以我们就保持足够低的图像分辨率和最大视差范围，连续的细化深度图，以确保最小的计算时间。

首先以最低分辨率（1/16的原分辨率）at full disparity range 估计视差图，这个阶段通常在几毫秒内就能完成。然后从低分辨率开始，通过上采样和随后纠正在更高分辨率下明显的错误来连续提高视差图的分辨率。通过使用 CNN 预测来自输入图像的上采样视差图的残差来执行校正。尽管使用了更高的分辨率，但这些更新仍然很快，因为可以假设剩余视差限制在几个像素内，从而允许我们将最大视差和相关计算限制在整个范围的仅 10 -20%。这些连续的更新完全避免了全范围视差计算，但最初的低分辨率设置，并确保所有计算都被重复使用。可以随时对我们的算法进行轮询，以检索当前最佳估计的深度图。可以获得广泛的可能帧速率 (英伟达TX2上的10-35FPS)，同时在高延迟设置中仍保留准确的视差估计
<br/>
<br/>
<br/>
<br/>

# 网络结构
<div align=center><img src="/posts/anynet/network.png" width="  "></div>

- 首先通过U-Net（U-Net feature extractor），在输入的一对图像中进行特征提取，分别在不同的几个分辨率下(1/16, 1/8, 1/4) 输出特征图。在stage1，只有最低分辨率的特征图（1/16）被输入到Disparity Net以产生一个低分辨率的视差图。由于低分辨率输入，整个第一阶段计算只需要几毫秒。
- 如果允许更多的计算时间，我们进入stage2，继续通过U-Net获得分辨率稍大（1/8,）的特征图。在stage2中，我们无需用刚得到的具有更高分辨率的特征图重新计算完整的视差图，而是简单地校正了stage1中已经计算出的视差图。首先，我们放大视差图（up-scale）以匹配stage2的分辨率。然后，我们计算一个残差图，其中包含小的校正，这些校正指定每个像素应增加或减少多少。
- 如果时间允许，进入stage3，过程类似stage2 ，并将分辨率再次从 1/8 倍增至 1/4。 
- Stage4 使用 SPNet 改进stage3的视差图。
<br/>
<br/>
<br/>
<br/>

# 实验与结论
<div align=center><img src="/posts/anynet/results.png" width="  "></div>

- 对照组：StereoNet、PSMNet、传统双目估计算法（OpenCV）
- 实验组：StereoNet
<br/>
<br/>
<br/>
<br/>

## 结论

- 只有 AnyNet 和 StereoNet 能够以≥30 FPS （GPU：英伟达TX2）的速度进行快速实时预测，并且 AnyNet 在两个数据集上的错误率都大大降低。 
- 即使使用全分辨率输入，AnyNet 也能够以超过 10 FPS 的速度运行，并且在每个可能的推理时间范围内，AnyNet 在预测误差方面明显优于所有对照组。  
- PSMNet 能够产生总体上最准确的结果，但这仅适用于 1 FPS 或更慢的计算速率。 
- 相比较之下非基于 CNN 的方法 OpenCV 在任何推理时间范围内都没有竞争力。



