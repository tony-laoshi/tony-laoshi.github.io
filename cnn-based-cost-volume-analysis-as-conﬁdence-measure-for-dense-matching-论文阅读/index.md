# CNN Based Cost Volume Analysis as Conﬁdence Measure for Dense Matching 论文阅读



- [**_Max Mehltretter, Christian Heipke_**; Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 0-0](https://openaccess.thecvf.com/content_ICCVW_2019/html/3DRW/Mehltretter_CNN-Based_Cost_Volume_Analysis_as_Confidence_Measure_for_Dense_Matching_ICCVW_2019_paper.html)
<br />
<br />
<br />
<br />


# 什么是置信度？
置信度能够识别密集立体匹配中的错误的视差分配（视差估计），对错误的视差估计的一种度量。因此被广泛利用于自动驾驶（需要高度信心作为强制性前提）。

- 近年来，基于深度学习的置信度估计方法在该领域表现亮眼，但是这些方法大多仅仅依赖于从视差图中学习特征而不考虑3D代价量。
- 而这些3D代价量是已被证明，如果被利用起来（相当于利用了额外的信息）是可以进一步的提高估计精度的。

**因此，该论文工作中，提出了一种新的CNN结构，能够从3D代价量中直接学习用于置信度估计的特征。**
<br />
<br />
<br />
<br />

# 什么是置信度估计？

- 通过立体匹配获取度信息是摄影测量学和计算机视觉中的经典任务。密集立体匹配是确定图像对中的每个或大多数像素的深度。原则上，深度重建可以被解释为透视投影的逆操作，这直接导致了该任务的主要困难：将3D场景投影到2D图像平面会导致降维。因此，逆运算一般没有唯一解，其特征是不适定。
- 然而，为了确定解决方案，识别一对图像中的两个像素的对应关系通常是先决条件（左图中p像素在右图中何处）。也就是视差估计的过程。
- 考虑到视差估计的可靠性。特别是在具有挑战性的条件下（纹理不够，相机移动快，图像质量不好等），深度重建方法可能无法识别所有像素的正确对应。**通俗来说，就是视差估计得不准，存在不确定性，那么度量出这种不确定性的任务就是置信度估计。**
<br />
<br />
<br />
<br />


# 置信度估计分类
从原理上置信度估计的方法可以被分为三类：

1. 基于单独的、人手工制作的特征（特征曲线、左右视差估计一致性、像素在其本地领域的独特性）；
1. 结合 1 中的某些特征来构造更鲁棒、准确的测量；
1. 把整个任务都映射到CNN中（用深度学习来估计）；

分析不同的CNN-based的工作中的优点，有两个要点是值得注意的：

1. **从代价量构造的特征表现出优越的性能**
1. **学习的特征胜过手工制作的特征**

有工作将这两个假设结合起来，从代价量中学习得到特征。虽然他们提出了不同的方法，但都包含一个预处理步骤来提取数据的子集，这些数据作为输入提供给他们的网络。他们指出，这样的预处理是必要的，因为原始成本量的成本分布通常不允许区分正确和不正确的估计。然而，所提出的预处理步骤限制了置信度估计步骤所提供的信息。这就阻止了该方法在成本量上充分利用学习特性的潜力。
<br />
<br />
<br />
<br />

# 基于代价量的置信度估计
基于代价量的置信度估计可以被认为是对 volumetric 3D data 的回归任务，因为它是单位间隔内实数的预测。在文献中，主要存在两种类型的方法来处理体积3D数据：

- 基于投影的方法
- 基于体素的处理

前者基于使用完善的网络体系结构将3D数据投影到一个或多个2D图像并在2D中应用分类的想法。得益于对2D图像分类的广泛研究，与基于体素的方法相比，这些方法在许多应用中表现出了卓越的性能。但是，他们主要根据对象的形状和表面对样本进行分类。对于当前评估代价量的任务，这是不合理的，因为这些代价量始终具有相同的形状，并且仅各个小体素中的值会变化。<br />基于体素的方法，直接从3D代价量学习特征，但是具有更高复杂性和更大内存消耗。
<br />
<br />
<br />
<br />

# 基于 CNN 的代价量分析
这项工作中提出的方法的主要思想是根据相应的代价量逐个像素地评估视差图的置信度。<br />3D代价量的 x 和 y 轴对应于图像坐标，而视差轴 z 表示相关的成本曲线。一般来说，可以在成本曲线上观察到典型特征，与它们的来源无关：在理想情况下，成本曲线包含一个唯一的最小值，成本为零，而所有其他值都处于最大值（图 a）。然而，在实践中，成本曲线通常有几个局部最小值，需要放宽理论假设。置信度高的视差估计的特征是明确且明确的全局最小值（图b）。相反，如果无法识别明显的全局最小值（图 c）或全局最小值宽且平坦（图 2d），则通常会分配低置信度，从而使正确对应的定位不准确。
<br />
<div align=center><img src="/posts/Conﬁdence_Measure_for_Dense_Matching/cost_curve.png" width="  "></div>

<br />代价曲线高度依赖于所使用的立体匹配方法。因此，统一的数据表示是直接从 3D 代价量学习估计置信度的先决条件。为此，方法的结果空间用于标准化成本量。结果是该范围内的实值的3D张量 [0,1]。

# 网络框架
<div align=center><img src="/posts/Conﬁdence_Measure_for_Dense_Matching/network.png" width="  "></div>
<br />
<br />
**CVA-Net**主要由三个部分组成：邻域融合，深度处理，分类。详细的网络框架由图3给出。作为输入，网络采用大小为 N ×N ×D 的cost volume extracts。这里解释一下，在许多立体匹配网络中，构造的cost volume的尺寸通常都比较大如H * W * D 或 1/2H * 1/2W * D，这里的成本体积提取的意思就是从一个大的cost volume 提取出一个小块。
感知场的大小设置为 N = 13 像素，在网络可用的信息量和生成的置信度图中的平滑度之间提供了良好的折衷。 为了包含完整的成本曲线，提取的深度 D 被选择为等于成本体积的深度。 在具体情况下，根据训练样本的理论最大视差，将一个体的深度设置为 256 个像素。
<div align=center><img src="/posts/Conﬁdence_Measure_for_Dense_Matching/network_layer.png" width="  "></div>
<br />
<br />

网络的第一部分，即邻域融合，将cost volume extracts中包含的信息合并为单个成本曲线。此过程背后的基本思想等同于大多数基于区域的匹配方法: 包括邻域信息可提高鲁棒性。特别是如果与感兴趣的像素相对应的成本曲线受到噪声的影响或提供了模糊的解决方案，则邻域信息是有益的。与网络的这一部分相关联的卷积核的深度被设置为3，以处理曲线的微小偏移，例如由成本计算步骤期间的离散化误差引起的。

在随后的深度处理部分，对合并后的成本曲线进行进一步处理，以得出表征曲线的高级特征。 值得注意的是，卷积核深度 d 随着层深度的增加而增加：从 $d=8$ 开始，每个新层的值都翻倍，直到达到 $d=64$。我们的实验表明，这种设计的性能略好于具有恒定卷积核深度的设计，同时必须学习的参数数量要少得多。 此外，padding用于网络深度处理部分的所有卷积。 这使输出张量的大小保持不变，并且与没有填充相比，提供了更多的特征作为后续置信度估计的输入。

网络的第三部分和最后一部分由全连接层组成，并执行最终的置信度估计。 为此，基于经过深度处理部分之后的特征，将视差估计二元分类为**正确**和**不正确**。 用于此分类的 sigmoid 非线性结果被解释为置信度并分配给初始成本体积提取的中心像素。

通过在最后一部分把FC层替换为卷积层,把整个结构转换为一个完全卷积网络。这可以使得训练的时候使用 image patch 进行训练，而在测试过程中，传入全分辨率的图像，进过一次前项传播，就得到全分辨率的confidence map。当然，也允许进行分段处理体积，如果硬件受限制。
<br />
<br />
# 实验部分
## 训练过程
在 KITTI 2012 数据集的前 20 个训练图像对上训练我们的网络。 为此，从cost volume中提取尺寸为 13 × 13 × 256 的张量。 每个cost volume extract 的中心都有ground truth。 使用不同数量的训练样本进行的实验表明，当在 KITTI 2012 数据集上进行训练时，我们网络的测试准确度收敛到大约 260 万个样本。 由于收敛点很大程度上取决于训练样本中存在的特征的方差，因此如果网络在不同数据上训练，这个数字可能会有很大差异。 

因为仅知道视差估计是否正确，所以估计其置信度的任务通常转移到二元分类任务。 当网络将视差估计分类为正确或不正确时，最终的 sigmoid 非线性结果被用作置信度分数。 这个二元的误差度量metric是：如果$|d_{est} - d_{gt}|<3\ pixels$或$|d_{est} - d_{gt}|<(d_{gt}*0.05)$，则认为视差估计$d_{est}$是正确的，$d_{gt}$是 ground truth。 

我们的网络在大小为 256 的批次上进行了 10 个 epoch 的训练，学习率为 10−4，然后是 3 个 epoch，学习率降低了 10 倍。卷积层初始化为正态分布 N(0, 0.0025)  ，对于全连接层，使用 Glorot 初始化 [4]。  Adam [10] 用于最小化二元交叉熵，将矩估计指数衰减率设置为其默认值 β1 =0.9 和 β2 =0.999。 最后，为了加强泛化，dropout以 0.5 的速率应用于全连接层。
<br />
<br />
## 实验结果和评价
该部分略，感兴趣的请自己查看原论文

