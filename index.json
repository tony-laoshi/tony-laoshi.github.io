[{"categories":["Dense Stereo Matching"],"content":"简介 在移动设备比如移动机器人，无人汽车上双目深度估计通常被期望能够在有限的计算资源下实时的估计出精确的视差图。现在的许多基于深度学习的双目深度估计的工作都需要在计算结果精度与整体效率之间做出权衡，而且网络参数很多。面对这些短板，AnyNet 可以在推理时权衡效率和准确性。深度估计是分阶段进行的，在此期间可以随时查询模型以输出其当前的最佳估计。 介绍 使用CNN来预测视差图时的计算复杂度通常和图像的分辨率成三次方，以及与所考虑的最大视差成线性关系。所以我们就保持足够低的图像分辨率和最大视差范围，连续的细化深度图，以确保最小的计算时间。 首先以最低分辨率（1/16的原分辨率）at full disparity range 估计视差图，这个阶段通常在几毫秒内就能完成。然后从低分辨率开始，通过上采样和随后纠正在更高分辨率下明显的错误来连续提高视差图的分辨率。通过使用 CNN 预测来自输入图像的上采样视差图的残差来执行校正。尽管使用了更高的分辨率，但这些更新仍然很快，因为可以假设剩余视差限制在几个像素内，从而允许我们将最大视差和相关计算限制在整个范围的仅 10 -20%。这些连续的更新完全避免了全范围视差计算，但最初的低分辨率设置，并确保所有计算都被重复使用。可以随时对我们的算法进行轮询，以检索当前最佳估计的深度图。可以获得广泛的可能帧速率 (英伟达TX2上的10-35FPS)，同时在高延迟设置中仍保留准确的视差估计 网络结构 首先通过U-Net（U-Net feature extractor），在输入的一对图像中进行特征提取，分别在不同的几个分辨率下(1/16, 1/8, 1/4) 输出特征图。在stage1，只有最低分辨率的特征图（1/16）被输入到Disparity Net以产生一个低分辨率的视差图。由于低分辨率输入，整个第一阶段计算只需要几毫秒。 如果允许更多的计算时间，我们进入stage2，继续通过U-Net获得分辨率稍大（1/8,）的特征图。在stage2中，我们无需用刚得到的具有更高分辨率的特征图重新计算完整的视差图，而是简单地校正了stage1中已经计算出的视差图。首先，我们放大视差图（up-scale）以匹配stage2的分辨率。然后，我们计算一个残差图，其中包含小的校正，这些校正指定每个像素应增加或减少多少。 如果时间允许，进入stage3，过程类似stage2 ，并将分辨率再次从 1/8 倍增至 1/4。 Stage4 使用 SPNet 改进stage3的视差图。 实验与结论 对照组：StereoNet、PSMNet、传统双目估计算法（OpenCV） 实验组：StereoNet ","date":"2022-09-28","objectID":"/anytime-stereo-image-depth-estimation-on-mobile-devices-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:0:0","tags":["Dense Stereo Matching"],"title":"Anytime Stereo Image Depth Estimation on Mobile Devices 论文阅读","uri":"/anytime-stereo-image-depth-estimation-on-mobile-devices-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":["Dense Stereo Matching"],"content":"结论 只有 AnyNet 和 StereoNet 能够以≥30 FPS （GPU：英伟达TX2）的速度进行快速实时预测，并且 AnyNet 在两个数据集上的错误率都大大降低。 即使使用全分辨率输入，AnyNet 也能够以超过 10 FPS 的速度运行，并且在每个可能的推理时间范围内，AnyNet 在预测误差方面明显优于所有对照组。 PSMNet 能够产生总体上最准确的结果，但这仅适用于 1 FPS 或更慢的计算速率。 相比较之下非基于 CNN 的方法 OpenCV 在任何推理时间范围内都没有竞争力。 ","date":"2022-09-28","objectID":"/anytime-stereo-image-depth-estimation-on-mobile-devices-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:1:0","tags":["Dense Stereo Matching"],"title":"Anytime Stereo Image Depth Estimation on Mobile Devices 论文阅读","uri":"/anytime-stereo-image-depth-estimation-on-mobile-devices-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":[" Confidence Estimation"],"content":"https://www.researchgate.net/publication/346096130_Aleatoric_Uncertainty_Estimation_for_Dense_Stereo_Matching_via_CNN-based_Cost_Volume_Analysis 摘要 在上篇文章中，我们知道了可以通过去分析 cost volume 来估计 confidence。得到的confidence map每一个像素是一个confidence 得分，解释为该像素位置的视差估计得多准。比如0.8就比0.5估计得准确。但是我们不能知道，这个不确定是多少？也就是说我们只知道这个像素位置的视差估计得不准确，但是具体有多不准确，我们无从得知。基于这个背景，作者在CVA-Net的基础上，提出了新的三种不确定性模型来量化不确定性（uncertainty）。 量化不确定性 为了准确地量化过程中固有的不确定性，有必要考虑所有潜在的不确定性来源。通常有两种类型的不确定性: 任意不确定性（aleatoric uncertainty）和认知不确定性（epistemic uncertainty）。 任意不确定性是由 natural variability 引起的，并包含在数据中。在这种情况下，natural variability 被理解为所考虑过程的可变、非确定性或简单的不可预测行为。相比之下，认知不确定性解释了对问题域的有限知识和用于设计或训练预测模型的简化。 在深度学习的背景下，不确定性通常也被区分为这两种情况。在设计或学习模型时，任意不确定性无法降低，因为它是固有的在数据中，因此独立于模型定义。但是，可以将认知不确定性最小化，以更多 (多样化) 训练数据的形式提供附加信息，从而可以形成更准确地表示基础过程的模型。然而，这两种不确定性之间的差异不仅可以在理论背景中看到，还可以在其量化的实际方法中看到：可以直接基于数据预测任意不确定性，另一方面，认知不确定性通常是基于抽样方法估计。 从密集立体匹配的角度来看，任意不确定性解释了传感器噪声、遮挡和匹配模糊等影响，这些影响是由场景中的无纹理区域或重复模式引起的。 另一方面，认知不确定性考虑简化匹配过程的假设，例如搜索范围的限制或理想情况下的极线校正，以及训练数据中缺少的特征，例如暗示某种几何形状的纹理和阴影形状。 请注意，在这项工作中，我们只关注任意不确定性的估计，认知不确定性不在考虑范围。 与仅使用二元分类模型的先前工作相比，这里所提出的 CNN 适用于并针对三种不同的不确定性模型进行训练：通过二元分类的置信度估计、残差学习和概率建模。 此外，除了对隐式假设以及优缺点进行理论讨论外，我们的网络的三个变体还基于实验进行了评估，扩展了对三种不同密集立体匹配方法已经详尽的评估。 网络模型 与原CVA-Net一致，但考虑的视差范围不同，原CVA-Net考虑的是256，这里考虑的是192。那么相应的cost volume 的深度的也应该为192。减少成本体积的深度，从而减少立体图像对中潜在对应的搜索范围，降低了面临模糊匹配结果的风险，这通常会导致更高准确度的视差图。 不确定性模型 虽然立体图像的视差估计通常是使用参考数据来学习的，但这些数据通常不适用于相关的不确定性。 尽管如此，为了能够学习不确定性估计的任务，通常会假设一个特定的不确定性模型，从而允许从估计的和ground truth视差之间的偏差中隐含地学习不确定性。 虽然在我们之前的工作中使用二元分类模型来学习置信度，但在这项工作中，我们讨论了两个额外的不确定性模型，并用于训练我们网络的不同变体：基于残差的模型和概率模型。 除了不同的损失函数外，可能还需要对最终网络层进行微调以满足三个变体的不同结果空间，这将在后续段落中详细解释。 ","date":"2022-09-28","objectID":"/aleatoric-uncertainty-estimation-for-dense-stereo-matching-via-cnn-based-cost-volume-analysis-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:0:0","tags":[" Confidence Estimation"],"title":"Aleatoric Uncertainty Estimation for Dense Stereo Matching via CNN Based Cost Volume Analysis 论文阅读","uri":"/aleatoric-uncertainty-estimation-for-dense-stereo-matching-via-cnn-based-cost-volume-analysis-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":[" Confidence Estimation"],"content":"二元分类 置信度是对视差估计的信任，它可以用这个估计正确的概率来表示。 因此，置信度估计通常被实现为二元分类问题：当网络将视差估计分类为正确或不正确时，正确的概率被用作置信度分数。 为了获得这个概率，我们的网络最后一层的结果被送到一个 sigmoid 非线性层。 二进制类标签的gt（ground truth）来自错误度量：$|d_{est} - d_{gt}|\u003c3\\ pixels$或者$|d_{est} - d_{gt}|\u003c(d_{gt}*0.05)$。 使用预测的置信度分数$\\gamma$和地面实况类别标签$\\hat{\\gamma}$，我们网络的第一个变体通过最小化加权二元交叉熵损失进行训练： $$ \\mathcal{L}_{BC} = \\frac{1}{N} \\sum_{i=1}^N w_i \\cdot H(\\gamma_i,\\hat{\\gamma}_i) $$ $$ w_i = \\hat{\\gamma_i} \\cdot (w_{corr}-1)+1 $$ $$ H(\\gamma, \\hat{\\gamma}) = -\\hat{\\gamma} \\cdot \\log (\\gamma)-(1-\\hat{\\gamma}) \\cdot \\log (1-\\gamma) $$ 其中$N$是有效视差的像素数（具有已知的GT视差的像素数），它们形成一个小批量，因此在一次前向传递中一起处理。 虽然函数$H$计算标准二元交叉熵，但具有正确视差估计的样本是由不正确和正确训练样本$w_{corr}$之间的比率加权。 损失函数中考虑了这个比率来解释不平衡的训练集，这会阻止网络学习更好地预测更频繁的类别。 ","date":"2022-09-28","objectID":"/aleatoric-uncertainty-estimation-for-dense-stereo-matching-via-cnn-based-cost-volume-analysis-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:1:0","tags":[" Confidence Estimation"],"title":"Aleatoric Uncertainty Estimation for Dense Stereo Matching via CNN Based Cost Volume Analysis 论文阅读","uri":"/aleatoric-uncertainty-estimation-for-dense-stereo-matching-via-cnn-based-cost-volume-analysis-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":[" Confidence Estimation"],"content":"残差模型 虽然置信度估计只允许评估视差分配是否正确，但误差的大小也可能令人感兴趣。 为了量化误差，训练网络以预测视差残差是一种常用方法，即像素的估计值与其真实视差之间的差异。 我们将这种方法应用于我们的第二个变体，将不确定性解释为视差预测误差的函数，这是由我们的 CNN 使用以下损失函数学习的 $$ \\mathcal{L}_{Res}= \\frac{1}{N} \\sum_{i=1}^N |\\Delta d_i-(\\hat{d}_i - d_i)|, $$ 其中$d$是估计的视差，$\\hat{d}$是GT视差。由CNN直接使用网络最终层的结果来预测视差残差$\\Delta d$，而无需应用任何非线性。 ","date":"2022-09-28","objectID":"/aleatoric-uncertainty-estimation-for-dense-stereo-matching-via-cnn-based-cost-volume-analysis-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:2:0","tags":[" Confidence Estimation"],"title":"Aleatoric Uncertainty Estimation for Dense Stereo Matching via CNN Based Cost Volume Analysis 论文阅读","uri":"/aleatoric-uncertainty-estimation-for-dense-stereo-matching-via-cnn-based-cost-volume-analysis-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":[" Confidence Estimation"],"content":"概率模型 对于第三个也是最后一个变体，学习预测任意不确定性的任务以贝叶斯方式解释。这通常是通过指定概率分布来描述数据中包含的不确定性来实现的，在训练过程中其可能性最大化。估计的视差和不确定性值用作此概率分布的参数，而GT视差用作观测值。使用这个公式，可以以隐式方式将任意不确定性学习为方差或标准偏差（取决于选择的概率分布），从而避免了对不确定性的参考的需要。基于L1范数在训练CNN进行视差回归任务的背景下的常见用法 ，我们使用Laplace分布来描述任意不确定性。为了能够使用通用优化器，我们基于深度学习的优化过程的目标被表述为这种分布的负对数似然 $$ -\\log p(\\hat{d}_i | d_i) \\propto \\frac{\\sqrt{2}}{\\sigma_i}|d_i-\\hat{d}_i|+\\log (\\sigma_i), $$ 其中，$d$是估计的视差，$\\hat{d}$是GT视差。$\\sigma$是假设的拉普拉斯分布的标准差，它表示各个像素的任意不确定性。同时在loss函数中替换$s = log(\\sigma)$，通过这种修改，我们的网络被训练用来预测对数标准差，这使得训练过程中在数值上更加稳定，并防止损失函数除以0，因此损失函数定义如下： $$ \\mathcal{L}_{Prob}=\\frac{1}{N} \\sum_{i=1}^N \\frac{\\sqrt{2}}{\\exp(s_i)}|d_i-\\hat{d}_i|+s_i $$ ","date":"2022-09-28","objectID":"/aleatoric-uncertainty-estimation-for-dense-stereo-matching-via-cnn-based-cost-volume-analysis-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:3:0","tags":[" Confidence Estimation"],"title":"Aleatoric Uncertainty Estimation for Dense Stereo Matching via CNN Based Cost Volume Analysis 论文阅读","uri":"/aleatoric-uncertainty-estimation-for-dense-stereo-matching-via-cnn-based-cost-volume-analysis-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":["Confidence Estimation"],"content":" Max Mehltretter, Christian Heipke; Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 0-0 什么是置信度？ 置信度能够识别密集立体匹配中的错误的视差分配（视差估计），对错误的视差估计的一种度量。因此被广泛利用于自动驾驶（需要高度信心作为强制性前提）。 近年来，基于深度学习的置信度估计方法在该领域表现亮眼，但是这些方法大多仅仅依赖于从视差图中学习特征而不考虑3D代价量。 而这些3D代价量是已被证明，如果被利用起来（相当于利用了额外的信息）是可以进一步的提高估计精度的。 因此，该论文工作中，提出了一种新的CNN结构，能够从3D代价量中直接学习用于置信度估计的特征。 什么是置信度估计？ 通过立体匹配获取度信息是摄影测量学和计算机视觉中的经典任务。密集立体匹配是确定图像对中的每个或大多数像素的深度。原则上，深度重建可以被解释为透视投影的逆操作，这直接导致了该任务的主要困难：将3D场景投影到2D图像平面会导致降维。因此，逆运算一般没有唯一解，其特征是不适定。 然而，为了确定解决方案，识别一对图像中的两个像素的对应关系通常是先决条件（左图中p像素在右图中何处）。也就是视差估计的过程。 考虑到视差估计的可靠性。特别是在具有挑战性的条件下（纹理不够，相机移动快，图像质量不好等），深度重建方法可能无法识别所有像素的正确对应。通俗来说，就是视差估计得不准，存在不确定性，那么度量出这种不确定性的任务就是置信度估计。 置信度估计分类 从原理上置信度估计的方法可以被分为三类： 基于单独的、人手工制作的特征（特征曲线、左右视差估计一致性、像素在其本地领域的独特性）； 结合 1 中的某些特征来构造更鲁棒、准确的测量； 把整个任务都映射到CNN中（用深度学习来估计）； 分析不同的CNN-based的工作中的优点，有两个要点是值得注意的： 从代价量构造的特征表现出优越的性能 学习的特征胜过手工制作的特征 有工作将这两个假设结合起来，从代价量中学习得到特征。虽然他们提出了不同的方法，但都包含一个预处理步骤来提取数据的子集，这些数据作为输入提供给他们的网络。他们指出，这样的预处理是必要的，因为原始成本量的成本分布通常不允许区分正确和不正确的估计。然而，所提出的预处理步骤限制了置信度估计步骤所提供的信息。这就阻止了该方法在成本量上充分利用学习特性的潜力。 基于代价量的置信度估计 基于代价量的置信度估计可以被认为是对 volumetric 3D data 的回归任务，因为它是单位间隔内实数的预测。在文献中，主要存在两种类型的方法来处理体积3D数据： 基于投影的方法 基于体素的处理 前者基于使用完善的网络体系结构将3D数据投影到一个或多个2D图像并在2D中应用分类的想法。得益于对2D图像分类的广泛研究，与基于体素的方法相比，这些方法在许多应用中表现出了卓越的性能。但是，他们主要根据对象的形状和表面对样本进行分类。对于当前评估代价量的任务，这是不合理的，因为这些代价量始终具有相同的形状，并且仅各个小体素中的值会变化。 基于体素的方法，直接从3D代价量学习特征，但是具有更高复杂性和更大内存消耗。 基于 CNN 的代价量分析 这项工作中提出的方法的主要思想是根据相应的代价量逐个像素地评估视差图的置信度。 3D代价量的 x 和 y 轴对应于图像坐标，而视差轴 z 表示相关的成本曲线。一般来说，可以在成本曲线上观察到典型特征，与它们的来源无关：在理想情况下，成本曲线包含一个唯一的最小值，成本为零，而所有其他值都处于最大值（图 a）。然而，在实践中，成本曲线通常有几个局部最小值，需要放宽理论假设。置信度高的视差估计的特征是明确且明确的全局最小值（图b）。相反，如果无法识别明显的全局最小值（图 c）或全局最小值宽且平坦（图 2d），则通常会分配低置信度，从而使正确对应的定位不准确。 代价曲线高度依赖于所使用的立体匹配方法。因此，统一的数据表示是直接从 3D 代价量学习估计置信度的先决条件。为此，方法的结果空间用于标准化成本量。结果是该范围内的实值的3D张量 [0,1]。 网络框架 **CVA-Net**主要由三个部分组成：邻域融合，深度处理，分类。详细的网络框架由图3给出。作为输入，网络采用大小为 N ×N ×D 的cost volume extracts。这里解释一下，在许多立体匹配网络中，构造的cost volume的尺寸通常都比较大如H * W * D 或 1/2H * 1/2W * D，这里的成本体积提取的意思就是从一个大的cost volume 提取出一个小块。 感知场的大小设置为 N = 13 像素，在网络可用的信息量和生成的置信度图中的平滑度之间提供了良好的折衷。 为了包含完整的成本曲线，提取的深度 D 被选择为等于成本体积的深度。 在具体情况下，根据训练样本的理论最大视差，将一个体的深度设置为 256 个像素。 网络的第一部分，即邻域融合，将cost volume extracts中包含的信息合并为单个成本曲线。此过程背后的基本思想等同于大多数基于区域的匹配方法: 包括邻域信息可提高鲁棒性。特别是如果与感兴趣的像素相对应的成本曲线受到噪声的影响或提供了模糊的解决方案，则邻域信息是有益的。与网络的这一部分相关联的卷积核的深度被设置为3，以处理曲线的微小偏移，例如由成本计算步骤期间的离散化误差引起的。 在随后的深度处理部分，对合并后的成本曲线进行进一步处理，以得出表征曲线的高级特征。 值得注意的是，卷积核深度 d 随着层深度的增加而增加：从 $d=8$ 开始，每个新层的值都翻倍，直到达到 $d=64$。我们的实验表明，这种设计的性能略好于具有恒定卷积核深度的设计，同时必须学习的参数数量要少得多。 此外，padding用于网络深度处理部分的所有卷积。 这使输出张量的大小保持不变，并且与没有填充相比，提供了更多的特征作为后续置信度估计的输入。 网络的第三部分和最后一部分由全连接层组成，并执行最终的置信度估计。 为此，基于经过深度处理部分之后的特征，将视差估计二元分类为正确和不正确。 用于此分类的 sigmoid 非线性结果被解释为置信度并分配给初始成本体积提取的中心像素。 通过在最后一部分把FC层替换为卷积层,把整个结构转换为一个完全卷积网络。这可以使得训练的时候使用 image patch 进行训练，而在测试过程中，传入全分辨率的图像，进过一次前项传播，就得到全分辨率的confidence map。当然，也允许进行分段处理体积，如果硬件受限制。 实验部分 ","date":"2022-08-22","objectID":"/cnn-based-cost-volume-analysis-as-con%EF%AC%81dence-measure-for-dense-matching-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:0:0","tags":["Confidence Estimation"],"title":"CNN Based Cost Volume Analysis as Conﬁdence Measure for Dense Matching 论文阅读","uri":"/cnn-based-cost-volume-analysis-as-con%EF%AC%81dence-measure-for-dense-matching-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":["Confidence Estimation"],"content":"训练过程 在 KITTI 2012 数据集的前 20 个训练图像对上训练我们的网络。 为此，从cost volume中提取尺寸为 13 × 13 × 256 的张量。 每个cost volume extract 的中心都有ground truth。 使用不同数量的训练样本进行的实验表明，当在 KITTI 2012 数据集上进行训练时，我们网络的测试准确度收敛到大约 260 万个样本。 由于收敛点很大程度上取决于训练样本中存在的特征的方差，因此如果网络在不同数据上训练，这个数字可能会有很大差异。 因为仅知道视差估计是否正确，所以估计其置信度的任务通常转移到二元分类任务。 当网络将视差估计分类为正确或不正确时，最终的 sigmoid 非线性结果被用作置信度分数。 这个二元的误差度量metric是：如果$|d_{est} - d_{gt}|\u003c3\\ pixels$或$|d_{est} - d_{gt}|\u003c(d_{gt}*0.05)$，则认为视差估计$d_{est}$是正确的，$d_{gt}$是 ground truth。 我们的网络在大小为 256 的批次上进行了 10 个 epoch 的训练，学习率为 10−4，然后是 3 个 epoch，学习率降低了 10 倍。卷积层初始化为正态分布 N(0, 0.0025) ，对于全连接层，使用 Glorot 初始化 [4]。 Adam [10] 用于最小化二元交叉熵，将矩估计指数衰减率设置为其默认值 β1 =0.9 和 β2 =0.999。 最后，为了加强泛化，dropout以 0.5 的速率应用于全连接层。 ","date":"2022-08-22","objectID":"/cnn-based-cost-volume-analysis-as-con%EF%AC%81dence-measure-for-dense-matching-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:1:0","tags":["Confidence Estimation"],"title":"CNN Based Cost Volume Analysis as Conﬁdence Measure for Dense Matching 论文阅读","uri":"/cnn-based-cost-volume-analysis-as-con%EF%AC%81dence-measure-for-dense-matching-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":["Confidence Estimation"],"content":"实验结果和评价 该部分略，感兴趣的请自己查看原论文 ","date":"2022-08-22","objectID":"/cnn-based-cost-volume-analysis-as-con%EF%AC%81dence-measure-for-dense-matching-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:2:0","tags":["Confidence Estimation"],"title":"CNN Based Cost Volume Analysis as Conﬁdence Measure for Dense Matching 论文阅读","uri":"/cnn-based-cost-volume-analysis-as-con%EF%AC%81dence-measure-for-dense-matching-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":["SLAM"],"content":"相关链接 标题：Towards Robust Indoor Vision SLAM and Dense Reconstruction for Mobile Robots 作者：Zhang, W; Wang, S; Haala, N. 来源：ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences; Gottingen Vol. V-1-2022, (2022): 211-219. DOI：10.5194/isprs-annals-V-1-2022-211-2022 动机 当前视觉SLAM可以很好的构建各种室内场景，但大多应用于室内小空间，且通常在纹理良好的环境中。 在大型室内场景的实际应用中，它们缺乏鲁棒性，无法构建全局一致的地图。 为此，在这篇文章中作者提出了一种新颖的系统，可以稳健地解决现有视觉 SLAM 遇到的问题，例如弱纹理和长期漂移。 整体思路 通过结合来自车轮里程计的信息，可以在没有纹理的情况下平滑地预测机器人位姿。 通过对齐基于截断有符号距离函数 (TSDF) 的子图来利用几何线索，以最大限度地减少长期漂移。 为了重建更完整和准确的稠密图，利用颜色信息和全局束调整（global BA）的优化结果来细化传感器获得的深度图。 结果 在公共数据集和自收集数据集上验证了所提出方法的准确性和鲁棒性，并展示了每个模块的互补性。 基于高精度ground-truth的评估结果表明，轨迹估计的平均绝对轨迹误差（ATE）从21 cm提高到2 cm，重建地图的平均精度为8 cm。 介绍 在目前阶段激光SLAM表现出强大的性能，视觉SLAM相比较之下缺乏足够的鲁棒性，并且在具有挑战性的场景下容易失败（例如，高速运动、多房间场景、纹理少或者没有纹理区域）。但是由于视觉SLAM相比与前者更加的经济实惠且能提供更加丰富的3D信息，所以具备被探索的潜力，也备受研究者们的追逐。 作者选择 基于密集光流的方法 （ dense optical ﬂow-based method （DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras. Advances in neural information processing systems）） 作为他工作的 baseline。随着计算能力的提高和深度学习研究的进步，这种基于光流的方法成为一种新的有前景的视觉SLAM范式。 在此之前，SLAM 方法主要分为基于关键点的方法（特征点法）和基于光度一致性的直接方法（光流法、直接法）。 根据每种方法的原理和特点，各有千秋。 一方面，基于关键点的方法更容易针对全局最优进行优化。 但是，它仅限于使用一小部分图像信息，而将其余可能有用的信息放在一边。 另一方面，直接方法使用半密集像素信息而不是仅使用稀疏关键点，但最小化光度一致性损失是一个更复杂的问题，在优化过程中更容易陷入局部最小值 。相比之下，基于密集光流的方法通过将密集像素信息的重投影误差作为稳定的优化目标，结合了基于关键点和直接基于光度计的方法的优点。 应用于移动机器人的 SLAM 方法经常结合来自车轮编码器的测量值，这是一种支持移动机器人导航的标准传感器。 根据地面材料的不同，车轮里程表可能会遇到不同程度的滑动，从而导致不同的水平测量误差。 长时间整合位姿，误差累积相当大。 相比之下，短时间内的位姿整合，更具体地说，在两个相邻关键帧的时间段内，是相当可靠的。 因此，为了增强整个系统的鲁棒性，车轮里程计的相对 2D 位姿变换被添加为连续关键帧之间的附加约束。 这些约束增加了轨迹估计的平滑度，并且在相机视图中缺少纹理时特别有用。 在室内环境中移动时，移动机器人需要依靠稠密的 3D 地图进行导航和避障。 作者使用深度相机来获取3D信息，因为现成的深度相机系统变得越来越可靠。 此外，深度相机在无纹理的室内场景中是有益的，因为它提供了用于对齐的几何信息。 简单地累积所有帧的深度点云将导致大量的信息冗余，因为相邻帧的观测结果大部分重叠。 为此，作者选择 TSDF 作为稠密 3D 地图的表示。这是一种基于体素的表示。 落入同一体素的 3D 点会被融合。 在减少冗余的同时，通过加权平均将观察噪声的影响降至最低。作者在特定时间窗口内构建分离的 TSDF 子图，这些子图可以在几何上相互对齐，为全局束调整（global BA）提供额外的约束。 结果，累积的姿势漂移可以通过闭合的长期循环来减轻。 与密集的光流约束相比，由于显着的视角和基线偏差可能会错过潜在的闭环，子图配准不受视角变化的限制，并且可以在几何上稳健地对齐地图。 a) 白墙前的摄像头跟踪失败； b) 与车轮里程计测量融合后的平滑跟踪；c) 因错过大基线回环闭合而被错位污染的重建地图； d) 改进了带有子图配准的密集地图。 作者还提出了一个深度图细化策略（depth map reﬁnement strategy），以提高密集重建的准确性和完整性。 观察误差相对于深度范围呈二次方增加。 在进行远距离观测时，深度测量会受到明显的观测噪声的影响，这会导致重建地图中出现大量重影和伪影，从而影响地图精度和机器人导航。 为了提高深度图质量，我们使用来自 SLAM 后端的全局 BA 的现成结果。 关键帧的优化深度图（在SLAM后端全局BA获得的深度图被成为优化深度图）受益于相邻帧之间光流的密集匹配，并且由于增加信息的联合优化而具有更高的精度。 根据 degree of over-determination，可以得出每个优化深度像素的置信度值$\\omega$，在此基础上优化深度（optimized depth）与原始传感器深度自适应融合。 此外，我们应用双边求解器来增强传感器深度图的平滑度，它利用彩色图像中的外观信息来保持物体边缘的清晰度，同时平滑物体表面上的测量值（这里的意思是深度相机直接获得的深度图成为原始传感器深度，这个深度图经过一个双边求解器来增强深度图的平滑度，这样就得到了一个平滑后的深度图，用这个平滑后的深度图与在SLAM后端全局BA获得的优化深度图进行一个自适应融合，得到最终的细化深度图，融合方案如下公式所示）。 从左到右：彩色图像、传感器深度图、平滑后深度图、优化的深度图和最终自适应融合后的深度图。有改进的地方和有所改善的区域用红色矩形标记 主要贡献总结 一种新颖的鲁棒室内视觉 SLAM 框架，通过探索不同约束的相互补充性质，克服了短期纹理缺失和长期大基线循环闭合的挑战。 一种基于传感器原始深度误差分析和结合颜色线索与双边求解器和全局束调整优化结果的深度图细化策略。 在多个数据集和数百个序列上进行实验，以验证所提出系统的有效性。 根据高端激光扫描仪模型评估最终重建。 ","date":"2022-07-15","objectID":"/towards-robust-indoor-vision-slam-and-dense-reconstruction-for-mobile-robots-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:0:0","tags":["SLAM"],"title":"Towards Robust Indoor Vision SLAM and Dense Reconstruction for Mobile Robots 论文阅读","uri":"/towards-robust-indoor-vision-slam-and-dense-reconstruction-for-mobile-robots-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":["CNN"],"content":"简介 介绍深度学习训练好的模型在C++中如何部署以及使用不同的推理引擎进行推理。这里是用Pytorch训练好的模型为例（用什么学习框架学习得到的模型没关系，都有对应的模块导出模型的）。PyTorch 模型经常需要部署在 C++ 程序中，目前我知道的方法有三种（还有其他方式我没有尝试过）： LibTorch: PyTorch 官方 C++ 库 ONNX Runtime OpenCV: DNN 模块 ","date":"2022-07-14","objectID":"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E6%8E%A8%E7%90%86/:1:0","tags":["CNN, ONNX Runtime"],"title":"深度学习模型的部署和推理","uri":"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E6%8E%A8%E7%90%86/"},{"categories":["CNN"],"content":"ONNX Runtime ONNX Runtime 是一个跨平台推理和训练机器学习加速器。也就是说，训练好的模型导出为 ONNX 格式后，可以用 ONNX Runtime 进行推理。 ","date":"2022-07-14","objectID":"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E6%8E%A8%E7%90%86/:2:0","tags":["CNN, ONNX Runtime"],"title":"深度学习模型的部署和推理","uri":"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E6%8E%A8%E7%90%86/"},{"categories":["CNN"],"content":"模型导出 ONNX（Open Neural Network Exchange）是一种针对机器学习所设计的开放式的文件格式，用于存储训练好的模型。它使得不同的人工智能框架（如Pytorch、MXNet）可以采用相同格式存储模型数据并交互。 ONNX的规范及代码主要由微软，亚马逊，Facebook和IBM等公司共同开发，以开放源代码的方式托管在Github上。目前官方支持加载ONNX模型并进行推理的深度学习框架有： Caffe2, PyTorch, MXNet，ML.NET，TensorRT 和 Microsoft CNTK，并且 TensorFlow 也非官方的支持ONNX。 使用torch.onnx.export 可以直接保存模型为 onnx 格式，实例代码如下： def export(model, inputL=None, inputR=None): import onnx device = torch.device('cpu') model = model.module.to(device) model.eval() if inputL is None: dummyL = torch.randn((1, 3, 368, 1232)).to(device) else: dummyL = inputL.to(device) if inputR is None: dummyR = torch.randn((1, 3, 368, 1232)).to(device) else: dummyR = inputR.to(device) onnx_path = \"anynet2.onnx\" # onnx_path = \"anynet_with_spn.onnx\" torch.onnx.export( model, (dummyL, dummyR), onnx_path, opset_version=11, input_names=[\"imgL\", \"imgR\"], output_names=[\"disp1\", \"disp2\", \"disp3\",] # output_names=[\"disp1\", \"disp2\", \"disp3\", \"disp4\"] ) onnx.checker.check_model(onnx.load(onnx_path)) print(\"ONNX model exported to: \" + onnx_path) ","date":"2022-07-14","objectID":"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E6%8E%A8%E7%90%86/:2:1","tags":["CNN, ONNX Runtime"],"title":"深度学习模型的部署和推理","uri":"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E6%8E%A8%E7%90%86/"},{"categories":["CNN"],"content":"推理模型 如果使用 ONNX Runtime 进行推理的话，事先还需要安装 ONNX Runtime（使用Python 的话比较简单，使用C++的话略微复杂） ONNX Runtime 安装（C++） 安装的方式一般分从源码编译安装（可以参考这个，流程基本一致，MacOS源码编译onnxruntime）和使用预编译好的包 。 官网上的的Linux预编译包下载指引让人摸不着头脑，这里建议直接从Github Release页面下载。解压后你会发现预编译包里除了一些文档之外，只有头文件和二进制的库文件，没有任何包管理相关 (CMake、pkg-config 之类) 的配置文件。虽然源码 CMakeLists 中明明有 pkg-config 配置文件 .pc 的生成，但不知为何并没有被打包进预编译包。总之，拿到预编译包你没法直接通过 CMake/pkg-config 引入自己的工程。 所以，在 CMake 项目中无法通过 find_package 找到 ONNX Runtime。可以仿照这个仓库，使用 find_path 和 find_library 来查找： find_path(ONNX_RUNTIME_SESSION_INCLUDE_DIRS onnxruntime_cxx_api.h HINTS /usr/local/include/onnxruntime/core/session/) find_path(ONNX_RUNTIME_PROVIDERS_INCLUDE_DIRS cuda_provider_factory.h HINTS /usr/local/include/onnxruntime/core/providers/cuda/) find_library(ONNX_RUNTIME_LIB onnxruntime HINTS /usr/local/lib) add_executable(inference inference.cpp) target_include_directories(inference PRIVATE ${ONNX_RUNTIME_SESSION_INCLUDE_DIRS} ${ONNX_RUNTIME_PROVIDERS_INCLUDE_DIRS}) target_link_libraries(inference PRIVATE ${ONNX_RUNTIME_LIB}) 分别指定了包含路径、库路径、链接库。 或者也可以在预编译包的根目录下建立 share/cmake/onnxruntime 文件夹，在里面创建 onnxruntimeConfig.cmake 文件，内容为： #This will define the following variables: # onnxruntime_FOUND -- True if the system has the onnxruntime library # onnxruntime_INCLUDE_DIRS -- The include directories for onnxruntime # onnxruntime_LIBRARIES -- Libraries to link against # onnxruntime_CXX_FLAGS -- Additional (required) compiler flags include(FindPackageHandleStandardArgs) # Assume we are in \u003cinstall-prefix\u003e/share/cmake/onnxruntime/onnxruntimeConfig.cmake get_filename_component(CMAKE_CURRENT_LIST_DIR \"${CMAKE_CURRENT_LIST_FILE}\" PATH) get_filename_component(onnxruntime_INSTALL_PREFIX \"${CMAKE_CURRENT_LIST_DIR}/../../../\" ABSOLUTE) set(onnxruntime_INCLUDE_DIRS ${onnxruntime_INSTALL_PREFIX}/include) set(onnxruntime_LIBRARIES onnxruntime) set(onnxruntime_CXX_FLAGS \"\") # no flags needed find_library(onnxruntime_LIBRARY onnxruntime PATHS \"${onnxruntime_INSTALL_PREFIX}/lib\" ) add_library(onnxruntime SHARED IMPORTED) set_property(TARGET onnxruntime PROPERTY IMPORTED_LOCATION \"${onnxruntime_LIBRARY}\") set_property(TARGET onnxruntime PROPERTY INTERFACE_INCLUDE_DIRECTORIES \"${onnxruntime_INCLUDE_DIRS}\") set_property(TARGET onnxruntime PROPERTY INTERFACE_COMPILE_OPTIONS \"${onnxruntime_CXX_FLAGS}\") find_package_handle_standard_args(onnxruntime DEFAULT_MSG onnxruntime_LIBRARY onnxruntime_INCLUDE_DIRS) 然后将预编译包的 include、lib、share 三个文件夹拷到系统路径，或者注册用户包，就能在自己 CMake 项目中使用 find_package 找到 onnxruntime 了： # 增加opencv的依赖 FIND_PACKAGE( OpenCV REQUIRED ) # onnxruntime find_package( onnxruntime REQUIRED) include_directories(${onnxruntime_INCLUDE_DIRS}) ADD_EXECUTABLE( main main.cpp ) TARGET_LINK_LIBRARIES( main ${OpenCV_LIBS} ${onnxruntime_LIBRARIES} ) 使用 ONNX Runtime 推理 这里附上一段在C++中使用 ONNX Runtime 进行推理的代码。其中一定要注意进行推理之前，要进行正确的图像预处理。同时，推理得到的Tensor 之后，如需转换成其他格式，也需要进行合适的后处理。在我这段例子当中，是将模型推理得到的Tensor 转换成视差图也就是cv::Mat 格式 #include \u003calgorithm\u003e #include \u003cassert.h\u003e #include \u003ciostream\u003e #include \u003csstream\u003e #include \u003cvector\u003e #include \u003cstring\u003e #include \u003cpng.h\u003e #include \u003cexperimental_onnxruntime_cxx_api.h\u003e #include \u003connxruntime_c_api.h\u003e #include \u003cstdio.h\u003e #include \u003ctorch/torch.h\u003e #include \u003copencv2/dnn.hpp\u003e #include \u003copencv2/imgproc.hpp\u003e #include \u003copencv2/highgui.hpp\u003e using namespace std; using namespace cv; using namespace torch; // pretty prints a shape dimension vector std::string print_shape(const std::vector\u003cint64_t\u003e\u0026 v) { std::stringstream ss(\"\"); for (size_t i = 0; i \u003c v.size() - 1; i++) ss \u003c\u003c v[i] \u003c\u003c \"x\"; ss \u003c\u003c v[v.size() - 1]; return ss.str(); } // image pre-process Mat imagePreprocess(Mat sourceImg, Mat preprocessedImage, std::vector\u003cint64_t\u003e inputDims){ // step 1: Resize the image. cv::Mat resizedImageBGR, resizedImageRGB, resizedImage; cv::resize(sourceImg, resizedImageBGR, cv::Size(inputDims.at(3), inputDims.at(2)), cv::InterpolationFlags::INTER_CUBIC); // step 2: Convert the image to HWC RGB UINT8 format. cv::cvtColor(resizedImageBGR, resizedImageRGB, cv::ColorConversionCodes::COLOR_BGR2RGB); //","date":"2022-07-14","objectID":"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E6%8E%A8%E7%90%86/:2:2","tags":["CNN, ONNX Runtime"],"title":"深度学习模型的部署和推理","uri":"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E6%8E%A8%E7%90%86/"},{"categories":["CNN"],"content":"OpenCV DNN 参见官方文档，OpenCV DNN 模块能够直接加载 onnx 格式的模型，使用这个模块可以直接进行推理。但是缺点是有一些网络中自定义的操作，可能不被opencv dnn模块所支持。实例代码如下： #include \u003copencv2/opencv.hpp\u003e using namespace cv; using namespace dnn; int main() { const bool use_cuda = false; const std::string fn_image = \"cat.jpg\"; const std::string fn_model = \"super_resolution.onnx\"; // load and config model Net net = readNetFromONNX(fn_model); net.setPreferableBackend(DNN_BACKEND_OPENCV); net.setPreferableTarget(DNN_TARGET_CPU); // source image auto image = imread(fn_image, cv::IMREAD_GRAYSCALE); Mat blob; blobFromImage(image, blob, 1.0 / 255.0); // inference and output net.setInput(blob); auto output = net.forward(); int new_size[] = { output.size[2], output.size[3] }; output = output.reshape(1, 2, new_size); convertScaleAbs(output, output, 255.0); } ","date":"2022-07-14","objectID":"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E6%8E%A8%E7%90%86/:3:0","tags":["CNN, ONNX Runtime"],"title":"深度学习模型的部署和推理","uri":"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E6%8E%A8%E7%90%86/"},{"categories":["CNN"],"content":"Libtorch ","date":"2022-07-14","objectID":"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E6%8E%A8%E7%90%86/:4:0","tags":["CNN, ONNX Runtime"],"title":"深度学习模型的部署和推理","uri":"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E6%8E%A8%E7%90%86/"},{"categories":["CNN"],"content":"模型导出 如果使用 LibTorch 进行推理的话，那么需要事先将模型导出为 Torch Script (.pt 格式) 文件，参见官方教程。有两种方法：Tracing 和 Scripting。 Tracing 就是提供一个示例输入，让 PyTorch 跑一遍整个网络，将过程中的全部操作记录下来，从而生成 Torch Script 模型： x = torch.randn(batch_size, 1, 224, 224, requires_grad=True) traced_script_model = torch.jit.trace(torch_model, x) traced_script_model.save(\"super_resolution.pt\") Scripting 则是直接分析网络结构转化模型： traced_script_model = torch.jit.script(torch_model) traced_script_model.save(\"super_resolution.pt\") 这两种方法各有优缺点：如果模型正向传播的控制流跟输入相关，显然 Tracing 只能得到一种输入下的控制流，此时应该用 Scripting；而当模型使用了一些 Torch Script 不支持的特性，同时模型源码又无法修改时（如果能访问源码，Scripting 可以通过加入注释的方法忽略它们），Scripting 便无能为力了，此时只能考虑 Tracing。更多关于 Tracing 和 Scripting 的区别可以参考 [Mastering TorchScript](https://paulbridger.com/posts/mastering-torchscript/)。 ","date":"2022-07-14","objectID":"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E6%8E%A8%E7%90%86/:4:1","tags":["CNN, ONNX Runtime"],"title":"深度学习模型的部署和推理","uri":"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E6%8E%A8%E7%90%86/"},{"categories":["CNN"],"content":"推理部分 LibTorch 下载参见 PyTorch: Get Started。CMake 工程中使用 LibTorch 只需要加入 find_package(Torch REQUIRED)，并将自己的可执行文件/库链接到 ${TORCH_LIBRARIES} 即可。具体进行推理的 C++ 代码如下： #include \u003copencv2/opencv.hpp\u003e #include \u003ctorch/script.h\u003e torch::Tensor toTensor(const cv::Mat\u0026 image) { // convert 8UC1 image to 4-D float tensor CV_Assert(image.type() == CV_8UC1); return torch::from_blob(image.data, { 1, 1, image.rows, image.cols }, torch::kByte) .toType(torch::kFloat32) .mul(1.f / 255.f); } cv::Mat toMat(const torch::Tensor\u0026 tensor) { // convert tensor to 8UC1 image using namespace torch; Tensor t = tensor.mul(255.f).clip(0, 255).toType(kU8).to(kCPU).squeeze(); CV_Assert(t.sizes().size() == 2); return cv::Mat(t.size(0), t.size(1), CV_8UC1, t.data_ptr()).clone(); } int main() { const bool use_cuda = false; const std::string fn_image = \"cat.jpg\"; const std::string fn_model = \"super_resolution.pt\"; // load model auto module = torch::jit::load(fn_model); if (use_cuda) module.to(torch::kCUDA); // load source image auto image = imread(fn_image, cv::IMREAD_GRAYSCALE); auto input = toTensor(image); if (use_cuda) input = input.to(torch::kCUDA); // inference auto output = module.forward({ input }).toTensor(); auto result_torch = toMat(output); imwrite(\"result_torch.png\", result_torch); } 参考链接 pytorch模型部署 深度学习之从 python 到 C++ ","date":"2022-07-14","objectID":"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E6%8E%A8%E7%90%86/:4:2","tags":["CNN, ONNX Runtime"],"title":"深度学习模型的部署和推理","uri":"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E6%8E%A8%E7%90%86/"},{"categories":["opencv"],"content":"前言 传统的立体匹配算法当中，BM（Block Matching），SGBM（Semi-Global Block matching），是两种常用的算法。并且这两种算法都在opencv中已经有了良好的实现。算法速度上BM \u003e SGBM，匹配精度上：BM \u003c SGBM。 代码实例 #include \u003ciostream\u003e #include \u003copencv2/core.hpp\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/highgui.hpp\u003e #include \u003cchrono\u003e //for stereo matching #include\u003copencv2/calib3d.hpp\u003e //for point cloud visualization #include \u003cpcl/point_cloud.h\u003e #include \u003cpcl/visualization/cloud_viewer.h\u003e using namespace std; using namespace cv; int main(int argc, char **argv){ // load RGB images Mat imgL = imread(\"/home/codes/opencvTest/kittiTestImages/imgL/000000_10.png\", 1); Mat imgR = imread(\"/home/codes/opencvTest/kittiTestImages/imgR/000000_10.png\", 1); //keep left rgb image Mat rgb = Mat(imgL); // convert rgb images to grayscale cvtColor(imgL, imgL, CV_RGB2GRAY); cvtColor(imgR, imgR, CV_RGB2GRAY); int width = imgL.cols; int height = imgL.rows; // set parameters // 每个参数的意义和调整设计SGBM算法背后的原理，这里不做解释，可自行查阅 int minDisparity = 0; int numDisparities = 92; //max disparity - min disparity int blockSize = 9; int P1 = 8 * blockSize*blockSize; int P2 = 32 *blockSize*blockSize; int disp12MaxDiff = 1; int preFilterCap = 63; int uniquenessRatio = 10; int speckleWindowSize = 100; int speckleRange = 32; int mode = StereoSGBM::MODE_SGBM; //init stereoSGBM cv::Ptr\u003cStereoSGBM\u003e sgbm = StereoSGBM::create(minDisparity, numDisparities, blockSize, P1, P2, disp12MaxDiff, preFilterCap, uniquenessRatio, speckleWindowSize, speckleRange, mode); //computer disparity Mat imgDisparity16S = Mat(height, width, CV_16S); chrono::steady_clock::time_point startTime = chrono::steady_clock::now(); sgbm-\u003ecompute(imgL, imgR, imgDisparity16S); std::chrono::steady_clock::time_point endTime = std::chrono::steady_clock::now(); double computerDuration = chrono::duration_cast\u003cstd::chrono::duration\u003cdouble\u003e \u003e(endTime - startTime).count(); cout \u003c\u003c \"Time of computer disparity: \" \u003c\u003c computerDuration \u003c\u003c endl; //转换成32F格式获得真实视差值，这里记得缩放 Mat imgDisparity32F = Mat(height, width, CV_32F); imgDisparity16S.convertTo(imgDisparity32F, CV_32F, 1.0 / 16); //如果想看下获得的视差图效果 /* Mat imgDispMap8U = Mat(height, width, CV_8U); double max, min; Point minLoc, maxLoc; minMaxLoc(imgDisparity32F, \u0026min, \u0026max, \u0026maxLoc, \u0026minLoc); double alpha = 255.0 / (max - min); imgDisparity32F.convertTo(imgDispMap8U, CV_8U, alpha, -alpha * min); cv::Mat colorisedDispMap; cv::applyColorMap(imgDispMap8U, colorisedDispMap, cv::COLORMAP_JET); imshow(\"color_32F-8U\", colorisedDispMap); cv::waitKey(0); */ } 视差图可视化 通过SGBM算法获得的视差图效果如下，由于SGBM算法的参数众多，所以调整不同的参数或许有改善的空间，具体操作可自行查阅。 原双目图像rgb左图 通过SGBM算法立体匹配获得的视差图。 左边有一部分没有视差是正常的，那是右图中看不到的部分，这部分的宽窄与SGBM算法参数设置有关 进行空洞值插值处理 从视差图得到直观效果来说貌似还可以。但是在SLAM中，拥有视差图是为后面映射为点云做准备的，所以这里将2D视差图映射成3D点云看看效果 点云可视化 点云正面 点云侧面 正面看着效果勉强还行，但是实际上并不如此。缩放比例，同时旋转下方位，从侧面观察可以看出，其实估计点云模型存在着放射，发散等问题。原因是因为存在较多无效点，需要经过剔除处理。 附带代码 float fx = 718.856; float fy = 718.856; float cx = 607.1928; float cy = 185.2157; float baseline = 0.3861; pcl::PointCloud\u003cpcl::PointXYZRGB\u003e::Ptr pointcloud(new pcl::PointCloud\u003cpcl::PointXYZRGB\u003e); for (int v = 0; v \u003c imgL.rows; v++) { for (int u = 0; u \u003c imgL.cols; u++) { if (imgDisparity32F.at\u003cfloat\u003e(v, u) \u003c= 10 || imgDisparity32F.at\u003cfloat\u003e(v, u) \u003e= 96) continue; pcl::PointXYZRGB point; double x = (u - cx) / fx; double y = (v - cy) / fy; double depth = fx * baseline / (imgDisparity32F.at\u003cfloat\u003e(v, u)); point.x = x * depth; point.y = y * depth; point.z = depth; point.b = rgb.at\u003ccv::Vec3b\u003e(v, u)[0]; point.g = rgb.at\u003ccv::Vec3b\u003e(v, u)[1]; point.r = rgb.at\u003ccv::Vec3b\u003e(v, u)[2]; pointcloud-\u003epush_back(point); } } pcl::visualization::CloudViewer viewer(\"viewer\"); while(1){ viewer.showCloud(pointcloud); } void insertDepth32f(cv::Mat\u0026 depth) { const int width = depth.cols; const int height = depth.rows; float* data = (float*)depth.data; cv::Mat integralMap = cv::Mat::zeros(height, width, CV_64F); cv::Mat ptsMap = cv::Mat::zeros(height, width, CV_32S); double* integral = (double*)integralMap.data; int* ptsIntegral = (int*)ptsMap.data; memset(integral, 0, sizeof(double) * width * height); me","date":"2022-07-14","objectID":"/opencv-sgbm%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95/:0:0","tags":["SGBM"],"title":"Opencv SGBM立体匹配算法","uri":"/opencv-sgbm%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95/"},{"categories":["opencv"],"content":" 数据类型的形式 数据类型的形式一般为CV_\u003cbit_depth\u003e(S|U|F)C\u003cnumber_of_channels\u003e，其各自的含义如下： bit_depth：比特数，代表图像像素的位数，即像素深度，比如8bite、16bites、32bites、64bites S|U|F: S代表signed int 有符号整形。U代表 unsigned int 无符号整形。F代表 float 单精度浮点型 C\u003cnumber_of_channels\u003e: 代表一张图片的通道数比如: channels = 1：灰度图片。是单通道图像 channels = 3：RGB彩色图像。是3通道图像 channels = 4：带alpha通道的RGB图像，表示透明度。是4通道图像 例如：CV_8U 代表的是像素位数为8，无符号数据，单通道格式 常见Opencv的数据类型 cv类型 枚举数值 空间大小 范围 常规类型 CV_8U 0 8bits 0~255 unsigned char或uint8_t CV_8S 1 8bits -128~127 char或int8_t CV_16U 2 16bits 0~65535 ushort, unsigned short int, unsigned short或uint16_t CV_16S 3 16bits -32768~32767 short, short int或int16_t CV_32S 4 32bits -2147483648~2147483647 int,long或int32_t/int64_t CV_32F 5 32bits 1.18e-38~3.40e38 float CV_64F 6 64bits 2.23e-308~1.79e308 double 类型 C1 C2 C3 C4 CV_8U 0 8 16 24 CV_8S 1 9 17 25 CV_16U 2 10 18 26 CV_16S 3 11 19 27 CV_32S 4 12 20 28 CV_32F 5 13 21 29 CV_64F 6 14 22 30 Mat 的创建 常使用的是利用构造函数 Mat(nrows, ncols, type, [fillValue])，最后一个参数中括号里代表可选参数，表示填入的初始值，如果不写默认为0。nrows、ncols一般为int型的整数，但ncols也可以为数组，这样表示建立一个多维Mat,此时传入的nrows表示维度。前两个参数还可以用Size(ncol,nrow)代替(注意Size里行列的顺序和函数参数的顺序是反的)，这就是另一种构造函数了。 也可以先声明一个Mat，然后利用其成员函数创建Mat。create()成员函数格式如下：Mat(nrows,ncols,type)，它只能接收3个参数，与构造函数的区别就是无法赋初值。使用示例如下： cv::Mat m1(4, 3, CV_8UC1); cv::Mat m2(3, 5, CV_8UC1, 200); cv::Mat m4; m4.create(2, 3, CV_8UC1); Mat 常用属性和函数 rows：返回Mat的行数，仅对二维Mat有效，高维返回-1 cols：返回Mat的列数，仅对二维Mat有效，高维返回-1 size：返回Mat的尺寸大小(长x宽x高…) dims：返回Mat中数据的维度，OpenCV中维度永远大于等于2。如 3 * 4 的矩阵为 2 维， 3 * 4 * 5 的为3维 depth()：用来度量每一个像素中每一个通道的精度，但它本身与图像的通道数无关！depth数值越大，精度越高。在Opencv中，Mat.depth()得到的是一个0~6的数字，分别代表不同的位数，对应关系如下： enum{CV_8U=0,CV_8S=1,CV_16U=2,CV_16S=3,CV_32S=4,CV_32F=5,CV_64F=6} channels()：通道数量，矩阵中表示一个元素所需要的值的个数。Mat矩阵元素拥有的通道数。例如常见的RGB彩色图像，channels==3；而灰度图像只有一个灰度分量信息，channels==1。 at()：返回Mat某行某列的元素(可修改) tr()：返回指向Mat的行指针 clone()：返回Mat的深拷贝 copyTo()：返回Mat的深拷贝 eye()：生成单位阵 zeros()：生成元素全为0的矩阵 ones()：生成元素全为1的矩阵 type()：返回Mat的元素类型索引 inv()：Mat求逆 mul()：Mat矩阵乘法 data()：返回指向矩阵数据单元的指针 total()：返回矩阵中的元素总数 cv::hconcat()：水平拼接两个矩阵 cv::vconcat()：数值拼接两个矩阵 Mat的元素操作 获取Mat中的元素可以使用成员函数 at()。.at\u003c\u003e() 用法是尖括号\u003c\u003e传入模板参数，表示数据类型，小括号()中再传入行、列的索引，返回元素的引用。也可以只传入行或列，这样就是整行或者整列。at()函数传入的模板参数数据类型必须与Mat的数据类型严格一致。 cv::Mat m1 = cv::Mat::eye(3, 3, CV_32F); m1.at\u003cfloat\u003e(1,1) = 2.0; 也可以使用ptr()成员函数。它返回的是一个指针。 Mat M = Mat::eye(10, 10, CV_64F); double sum = 0; for (int i = 0; i \u003c M.rows; i++) { const double *Mi = M.ptr\u003cdouble\u003e(i); for (int j = 0; j \u003c M.cols; j++) sum += std::max(Mi[j], 0.); } 图像的显示 imshow函数在显示图像时，会将各种类型的数据都映射到[0, 255]。 比如：如果载入的图像是8U类型，就显示图像本来的样子。如果图像是16U 或32S（32S去掉符号位只有16位），便用像素值除以256。也就是说，值的范围是 [0, 255*256]映射到[0,255]。如果图像是32位或64位浮点型（32For 64F），像素值便要乘以255。也就是说，该值的范围是 [0,1]映射到[0,255]。 如：CV_8U的灰度或BGR图像的颜色分量都在0~255之间。直接imshow可以显示图像。CV_32F或者CV_64F取值范围为0~1.0，imshow的时候会把图像乘以255后再显示。 图像的尺寸操作 #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e using namespace cv; int main() { Mat srcImage = imread(\"1.jpg\"); Mat temImage, dstImage1, dstImage2; temImage = srcImage; //尺寸调整 //第一个参数：输入图像 //第二个参数：输出图像 //第三个参数输出图像的尺寸，如果是0，则有dsize=Size(round(fx*src.cols),round(fy*src,rows))计算得出 //第四个参数：水平轴的缩放系数，默认为0 //第五个参数：y轴撒谎能够的缩放系数，默认为0 //第六个参数：插值方式，默认为INTER_LINEAR线性插值 resize(temImage, dstImage1, Size(temImage.cols / 2, temImage.rows / 2), 0, 0, INTER_LINEAR); resize(temImage, dstImage2, Size(temImage.cols * 2, temImage.rows * 2), 0, 0, INTER_LINEAR); imshow(\"缩小\", dstImage1); imshow(\"放大\", dstImage2); waitKey(); return 0; 图像标准化操作(减去均值，除以方差) cv::Mat Normalizer(cv::Mat src, cv::Mat dst){ std::vector\u003cfloat\u003e mean = {0.485, 0.456, 0.406}; std::vector\u003cfloat\u003e std = {0.229, 0.224, 0.225}; std::vector\u003ccv::Mat\u003e bgrChannels(3); cv::split(src, bgrChannels); for (auto i = 0; i \u003c bgrChannels.size(); i++) { bgrChannels[i].convertTo(bgrChannels[i], CV_32FC1, 1.0 / std_value[i], (0.0 - mean_value[i]) / std_value[i]); } cv::meger(bgrChannels, dst); } 数据类型之间的转换 void convertTo( OutputArray m, int rtype, double alpha=1, double beta=0 ) const; 描述：把一个矩阵从一种数据类型转换到另一种数据类型，同时可以带上缩放因子和增量。 参数解释 m 目标矩阵。如果m在运算前没有合适的尺寸或类型，将被重新分配。 rtype 目标矩阵的类型。因为目标矩阵的通道数与源矩阵一样，所以r","date":"2022-07-14","objectID":"/opencv%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/:0:0","tags":["cv::Mat"],"title":"Opencv图像数据类型","uri":"/opencv%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"},{"categories":["opencv"],"content":"介绍 归一化和标准化都是对数据做变换，将原始的一组数据转换到某个范围或者说到某种状态。在数据处理的时候我们常会碰到如下三种处理： 归一化（Normalization）：将一组数据变化到某个固定区间内，比较常见的如[0, 1]。广义来说可以是各种区间，比如在对图像进行处理是会经常映射到[0, 255]，也有映射到[-1, 1]的情况 标准化（Standardization）：将一组数据变换为均值为0，标准差为1的分布（该分布并非一定符合正态分布） 中心化：中心化也叫零均值处理，就是用一组原始数据减去这些数据的均值。比如在ICP算法中会先对数据进行中心化 联系和区别 本质上，归一化和标准化都是对数据做线性变换。 但是也存在一些区别。比如第一，归一化会严格的限定变换后数据的区间。而标准化没有严格的区间限定，只是其数据的均值为0，标准差为1。第二，归一化对数据的缩放比例仅仅和极值有关，比如100个数，你除去极大值和极小值其他数据都更换掉，缩放比例$\\alpha = X_{max}-X_{min}$是不变的；但是标准化中，如果除去极大值和极小值其他数据都更换掉，那么均值和标准差大概率会改变，这时候，缩放比例自然也改变了。 标准化和归一化的多种形式 广义的说，标准化和归一化同为对数据的线性变化，所以我们没必要规定死，归一化就是必须到[ 0 , 1]之间，我到[ 0 , 1 ]之间之后再乘一个255也没有问题对吧。常见的有以下几种： 归一化的最通用模式Normalization，也称线性归一化： $$X_{n e w}=\\frac{X_{i}-X_{\\min }}{X_{\\max }-X_{\\min }}, \\text { 范围 }[0,1]$$ 均值归一化 （Mean normalization）： $$X_{n e w}=\\frac{X_{i}-\\operatorname{mean}(X)}{X_{\\text {max }}-X_{\\min }} \\text {, 范围 }[-1,1]$$ 标准化(Standardization)，也叫标准差标准化： $$X_{n e w}=\\frac{X_{i}-\\mu}{\\sigma} \\text {, 范围实数集 }$$ 标准化、归一化的原因、用途 为何统计模型、机器学习和深度学习任务中经常涉及到数据(特征)的标准化和归一化呢，一半原因以下几点，当然可能还有一些其他的作用，大家见解不同，我说的这些是通常情况下的原因和用途。 统计建模中，如回归模型，自变量X 的量纲不一致导致了回归系数无法直接解读或者错误解读；需要将X都处理到统一量纲下，这样才可比； 机器学习任务和统计学任务中有很多地方要用到“距离”的计算，比如PCA，比如KNN，比如kmeans等等，假使算欧式距离，不同维度量纲不同可能会导致距离的计算依赖于量纲较大的那些特征而得到不合理的结果； 参数估计时使用梯度下降，在使用梯度下降的方法求解最优化问题时， 归一化/标准化后可以加快梯度下降的求解速度，即提升模型的收敛速度。 什么时候Standardization，什么时候Normalization 如果对输出结果范围有要求，用归一化 如果数据较为稳定，不存在极端的最大最小值，用归一化 如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响 cv::normalize() 函数介绍 void normalize(InputArray src, OutputArray dst, double alpha=1, double beta=0, int norm_type=NORM_L2, int dtype=-1, InputArray mask=noArray()); 参数 src - 输入数组 dst - 输出数组，支持原地运算 alpha - range normalization模式的最小值 beta - range normalization模式的最大值，不用于norm normalization(范数归一化)模式。 norm_type - 归一化的类型，可以有以下的取值 NORM_MINMAX:数组的数值被平移或缩放到一个指定的范围，线性归一化，一般较常用。比如归一化到[min, max]范围内，则计算公式如下： $$d s t(i, j)=\\frac{[\\operatorname{src}(i, j)-\\min (\\operatorname{src}(x, y))] *(\\max -\\min )}{\\max (\\operatorname{src}(x, y))-\\min (\\operatorname{src}(x, y))}+\\min$$ NORM_INF: 归一化数组的无穷范数(绝对值的最大值)。每个值除以最大值来进行无穷范数归一化。同上最终归一化的值为单位向量的每个值乘以参数要归一化的范数值alpha NORM_L1 : 归一化数组的L1-范数(绝对值的和)。数组元素绝对值求和，然后算出每一个元素比上总和的比值，加起来总为1。这里要归一化的范数值为1.0，所求出的比值即为最后归一化后的值，若归一化范数值alpha为2.0，则每个比值分别乘以2.0即得到最后归一化后的结果为0.2, 0.8, 1.0，以此类推 NORM_L2: 归一化数组的L2-范数(各元素的平方和然后求平方根 ，欧氏距离)。即将该向量归一化为单位向量，每个元素值除以该向量的模长。同上最终归一化的值为单位向量的每个值乘以参数要归一化的范数值alpha dtype - 为负值时, 输出数据类型和输入数据类型一致，否则和src通道一致，depth =CV_MAT_DEPTH(dtype) mask - 掩码。选择感兴趣区域，选定后只能对该区域进行操作。 参考链接 标准化和归一化，请勿混为一谈，透彻理解数据变换 ","date":"2022-07-14","objectID":"/opencv%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E5%BD%92%E4%B8%80%E5%8C%96/:0:0","tags":["opencv"],"title":"Opencv数据标准化和归一化","uri":"/opencv%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E5%BD%92%E4%B8%80%E5%8C%96/"},{"categories":["软件安装"],"content":"介绍 Git 是目前世界上被最广泛使用的现代软件版本管理系统。Git 本身亦是一个成熟并处于活跃开发状态的开源项目，它最初是由 Linux 操作系统内核的创造者 Linus Torvalds 在 2005 年创造。今天惊人数量的软件项目依赖 Git 进行版本管理，这些项目包括开源以及各种商业软件。Git 在职业软件开发者中拥有良好的声誉，Git 目前支持绝大多数的操作系统以及 IDE（Integrated Development Environments）。 快速指南 该快速指南是面向从零开始的读者。 ","date":"2022-07-14","objectID":"/git%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E4%BB%A5%E5%8F%8Avscode%E5%8F%AF%E8%A7%86%E5%8C%96git/:0:0","tags":["Git"],"title":"Git快速入门以及VScode可视化Git","uri":"/git%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E4%BB%A5%E5%8F%8Avscode%E5%8F%AF%E8%A7%86%E5%8C%96git/"},{"categories":["软件安装"],"content":"安装Git Linux ： sudo apt-get install git Git 的高质量中文教程 https://github.com/geeeeeeeeek/git-recipes.git vs code上 Git可视化教程 https://cloud.tencent.com/developer/article/1793472 ","date":"2022-07-14","objectID":"/git%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E4%BB%A5%E5%8F%8Avscode%E5%8F%AF%E8%A7%86%E5%8C%96git/:1:0","tags":["Git"],"title":"Git快速入门以及VScode可视化Git","uri":"/git%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E4%BB%A5%E5%8F%8Avscode%E5%8F%AF%E8%A7%86%E5%8C%96git/"},{"categories":["软件安装"],"content":"前言 pcl库的编译安装真心让人想吐，运气好一次通过，运气不好（各种环境的、各种依赖的问题）两三天就过去了。在这里分享下我安装pcl所遇到的问题。 通过sudo apt 安装 通过这种方式安装的最大的好处就是简单且不容易出现编译安装的问题，缺点就是可能会有部分功能无法使用 ","date":"2022-07-14","objectID":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/:0:0","tags":["PCL"],"title":"Ubuntu20.04安装PCL库","uri":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["软件安装"],"content":"安装依赖 sudo apt-get update sudo apt-get install git build-essential linux-libc-dev sudo apt-get install cmake cmake-gui sudo apt-get install libusb-1.0-0-dev libusb-dev libudev-dev sudo apt-get install mpi-default-dev openmpi-bin openmpi-common sudo apt-get install libflann1.9 libflann-dev sudo apt-get install libeigen3-dev sudo apt-get install libboost-all-dev sudo apt-get install libqhull* libgtest-dev sudo apt-get install freeglut3-dev pkg-config sudo apt-get install libxmu-dev libxi-dev sudo apt-get install mono-complete sudo apt-get install libopenni-dev sudo apt-get install libopenni2-dev sudo apt-get install libvtk7-dev libvtk6-dev sudo apt-get install qt5-default ","date":"2022-07-14","objectID":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/:1:0","tags":["PCL"],"title":"Ubuntu20.04安装PCL库","uri":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["软件安装"],"content":"安装PCL sudo apt-get install libpcl-dev 至此，PCL库的安装和配置就算是完成了，接下来测试一下PCL库是否可以正常运行 ","date":"2022-07-14","objectID":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/:2:0","tags":["PCL"],"title":"Ubuntu20.04安装PCL库","uri":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["软件安装"],"content":"测试 ","date":"2022-07-14","objectID":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/:3:0","tags":["PCL"],"title":"Ubuntu20.04安装PCL库","uri":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["软件安装"],"content":"编写源文件 写个测试用的源文件 test.cpp （网上copy的） #include \u003ciostream\u003e #include \u003cpcl/common/common_headers.h\u003e #include \u003cpcl/io/pcd_io.h\u003e #include \u003cpcl/visualization/pcl_visualizer.h\u003e #include \u003cpcl/visualization/cloud_viewer.h\u003e #include \u003cpcl/console/parse.h\u003e int main(int argc, char **argv) { std::cout \u003c\u003c \"Test PCL !!!\" \u003c\u003c std::endl; pcl::PointCloud\u003cpcl::PointXYZRGB\u003e::Ptr point_cloud_ptr (new pcl::PointCloud\u003cpcl::PointXYZRGB\u003e); uint8_t r(255), g(15), b(15); for (float z(-1.0); z \u003c= 1.0; z += 0.05) { for (float angle(0.0); angle \u003c= 360.0; angle += 5.0) { pcl::PointXYZRGB point; point.x = 0.5 * cosf (pcl::deg2rad(angle)); point.y = sinf (pcl::deg2rad(angle)); point.z = z; uint32_t rgb = (static_cast\u003cuint32_t\u003e(r) \u003c\u003c 16 | static_cast\u003cuint32_t\u003e(g) \u003c\u003c 8 | static_cast\u003cuint32_t\u003e(b)); point.rgb = *reinterpret_cast\u003cfloat*\u003e(\u0026rgb); point_cloud_ptr-\u003epoints.push_back (point); } if (z \u003c 0.0) { r -= 12; g += 12; } else { g -= 12; b += 12; } } point_cloud_ptr-\u003ewidth = (int) point_cloud_ptr-\u003epoints.size (); point_cloud_ptr-\u003eheight = 1; pcl::visualization::CloudViewer viewer (\"test\"); viewer.showCloud(point_cloud_ptr); while (!viewer.wasStopped()){ }; return 0; } ","date":"2022-07-14","objectID":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/:3:1","tags":["PCL"],"title":"Ubuntu20.04安装PCL库","uri":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["软件安装"],"content":"编写CMakeLists.txt 在目录下创建TestPCL文件夹，用于存储测试项目的文件，将test.cpp和CMakeLists.txt存储至TestPCL文件夹，创建TestPCL/bulid文件夹以储存中间文件。 cmake_minimum_required(VERSION 2.6) project(TEST) find_package(PCL REQUIRED) include_directories(${PCL_INCLUDE_DIRS}) link_directories(${PCL_LIBRARY_DIRS}) add_definitions(${PCL_DEFINITIONS}) add_executable(TEST test.cpp) target_link_libraries (TEST ${PCL_LIBRARIES}) install(TARGETS TEST RUNTIME DESTINATION bin) ","date":"2022-07-14","objectID":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/:3:2","tags":["PCL"],"title":"Ubuntu20.04安装PCL库","uri":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["软件安装"],"content":"编译安装运行 cd build cmake .. make ./TEST ","date":"2022-07-14","objectID":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/:3:3","tags":["PCL"],"title":"Ubuntu20.04安装PCL库","uri":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["软件安装"],"content":"正常运行结果 通过源码安装 ","date":"2022-07-14","objectID":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/:3:4","tags":["PCL"],"title":"Ubuntu20.04安装PCL库","uri":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["软件安装"],"content":"安装PCL依赖 sudo apt-get update sudo apt-get install git build-essential linux-libc-dev sudo apt-get install cmake cmake-gui sudo apt-get install libusb-1.0-0-dev libusb-dev libudev-dev sudo apt-get install mpi-default-dev openmpi-bin openmpi-common sudo apt-get install libflann1.9 libflann-dev # 有说ubuntu16对应1.8，ubuntu18对应1.9，我直接用了1.9 sudo apt-get install libeigen3-dev sudo apt-get install libboost-all-dev sudo apt-get install libqhull* libgtest-dev sudo apt-get install freeglut3-dev pkg-config sudo apt-get install libxmu-dev libxi-dev sudo apt-get install mono-complete sudo apt-get install libopenni-dev sudo apt-get install libopenni2-dev ","date":"2022-07-14","objectID":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/:4:0","tags":["PCL"],"title":"Ubuntu20.04安装PCL库","uri":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["软件安装"],"content":"安装VTK ","date":"2022-07-14","objectID":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/:5:0","tags":["PCL"],"title":"Ubuntu20.04安装PCL库","uri":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["软件安装"],"content":"安装vtk依赖 #首先安装VTK的依赖：X11，OpenGL；cmake和cmake-gui在安装pcl依赖的时候安装过了的话可以跳过 sudo apt-get install libx11-dev libxext-dev libxtst-dev libxrender-dev libxmu-dev libxmuu-dev #OpenGL sudo apt-get install build-essential libgl1-mesa-dev libglu1-mesa-dev #cmake \u0026\u0026 cmake-gui sudo apt-get install cmake cmake-gui ","date":"2022-07-14","objectID":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/:5:1","tags":["PCL"],"title":"Ubuntu20.04安装PCL库","uri":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["软件安装"],"content":"下载vtk 官网下载链接：Download | VTK 这里要注意以下VTK 和 PCL 版本之间的兼容性。我不确切肯定有版本兼容的问题，只是在我尝试了不同VTK版本后，发现VTK版本和PCL的版本还是有一定要求的。最好安装两者的版本如下（自己尝试过）: ubuntu版本呢 pcl 版本 vtk版本 18.04 / 20.04 1.9.1 8.2.0 18.04 / 20.04 1.8.1 7.1.1 16.04 1.7.2 5.10.1 /6.2.0 ","date":"2022-07-14","objectID":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/:5:2","tags":["PCL"],"title":"Ubuntu20.04安装PCL库","uri":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["软件安装"],"content":"编译安装 下载完成之后解压到准备好的安装目录，再通过终端打开 cmake GUI 模式 cmake-gui 在cmake-gui中： 配置 where is the source code 为VTK-8.2.0所在目录。（然后在VTK-8.2.0所在目录下新建一个build文件夹） 配置 where to build the binaries 为VTK-8.2.0下的build文件夹 点击Configure，（用“Unix Makefiles”就可以）。配置完成后，显示“Configuring done” 勾选VTK-Group-Qt，VTK_QT_VERSION选为5，再点击Configure，配置完成后，显示“Configuring done” 点击Generate，显示“Generating done”，在build文件夹下生成工程文件 推出cmake-gui 在终端里切换到VTK-8.2.0安装目录下的build文件夹 make -j4 #性能好内存大的电脑就用 -j8 吧 sudo make install ","date":"2022-07-14","objectID":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/:5:3","tags":["PCL"],"title":"Ubuntu20.04安装PCL库","uri":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["软件安装"],"content":"安装PCL 官网下载连接：Releases · PointCloudLibrary/pcl 到PCL的github主页下载需要的版本(这里我下载的1.9.1)，放到准备好的安装目录下。打开终端，进到pcl的安装目录下： mkdir build cd build # 设置CMAKE_INSTALL_PREFIX是为了把pcl安装到指定文件夹内，所以这个路径根据自己的情况设置 cmake -DCMAKE_INSTALL_PREFIX=/usr/local/pcl-1.9.1 -DCMAKE_TYPE=None .. make -j4 #一样的，根据实际情况调整 sudo make install ","date":"2022-07-14","objectID":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/:6:0","tags":["PCL"],"title":"Ubuntu20.04安装PCL库","uri":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["软件安装"],"content":"测试 同上 ","date":"2022-07-14","objectID":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/:7:0","tags":["PCL"],"title":"Ubuntu20.04安装PCL库","uri":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["软件安装"],"content":"遇到的问题 ","date":"2022-07-14","objectID":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/:8:0","tags":["PCL"],"title":"Ubuntu20.04安装PCL库","uri":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["软件安装"],"content":"问题1 在make pcl的时候，报错：“/usr/bin/ld: cannot find -lvtkIOMPIImage /usr/bin/ld: cannot find -lvtkIOMPIParallel /usr/bin/ld: cannot find -lvtkFiltersParallelDIY2” 解决方案 重新编译安装 vtk。在cmake-gui模式下，完成第3步的Configure，然后勾选Advanced，在search中把/usr/bin/ld找不到的vtkIOMPIImage，vtkIOMPIParallel，vtkFiltersParallelDIY2都选上。包括如果出现了类似的问题也可以县看看这里面是否可以选上的。 再点击Configure，显示“Configuring done”，点击Generate，显示“Generating done”。 打开终端，完成 make 和 sudo make install ","date":"2022-07-14","objectID":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/:8:1","tags":["PCL"],"title":"Ubuntu20.04安装PCL库","uri":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["软件安装"],"content":"问题2 cmake pcl 的时候，提示Checking for module ‘metslib’ – No package ‘metslib’ found 解决方案 安装metslib（我用的是0.5.3版本） 下载链接：https://www.coin-or.org/download/source/metslib/metslib-0.5.3.tgz 解压后，在metslib-0.53打开命令终端并执行 sudo sh ./configure sudo make sudo make install 顺利完成安装metslib ","date":"2022-07-14","objectID":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/:8:2","tags":["PCL"],"title":"Ubuntu20.04安装PCL库","uri":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["软件安装"],"content":"问题3 No rule to make target in /usr/lib/x86_64-linux-gnu/libpcl_surface. so **后面的内容不记得了** 解决方案 lcoate libpcl_surface. so /usr/lib/x86_64-linux-gnu/libpcl_surface.so.1.10 结果没有上面说的libpcl_surface. so，这是因为我之前通过sudo apt install libpcl-dev 安装过pcl，所以动态库还保留在那里。 简而言之就是，要在/usr/lib/x86_64-linux-gnu/ 找libpcl_surface. so这个动态库。 所以先找到 libpcl_surface. so在哪里 whereis libpcl_surface. so libpcl_surface: /usr/lib/libpcl_surface.so 发现在/usr/lib/这个路径。所以只需要把这个路径下的动态库建立一个软链接到/usr/lib/x86_64-linux-gnu/就行。大概了，还有很多动态库都是这种情况，所以可以使用候补符号一次性解决。 sudo ln -s /usr/lib/libpcl_*.so /usr/lib/x86_64-linux-gnu 参考链接 二、PLC安装踩坑总结（Ubuntu 16.4+PCL1.8.1+VTK7.1+Qt5.9.9)_way7486chundan的博客-CSDN博客 ","date":"2022-07-14","objectID":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/:8:3","tags":["PCL"],"title":"Ubuntu20.04安装PCL库","uri":"/pcl%E5%BA%93%E5%9C%A8ubuntu20.04%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85/"},{"categories":["软件安装"],"content":"简介 Latex是一个基于TEX的排班系统。粘贴复制的话就不搞了。简而言之，通俗来说，就是用来写文章的，而且完全不需要你去花时间去自己考虑排版、页码等没必要花时间去弄的玩意。而且，对于数学公式的输入也是极为方便，这点在后面会展示。 安装 安装发行版 sudo apt-get install texlive-full 安装XeLaTeX编译引擎sudo apt-get install texlive-xetex 安装中文支持包（如果需要）sudo apt-get install texlive-lang-chinese 安装编辑器（图形化界面的选择有很多，例如 TeXStudio，TeXmaker等，可以看做是一个编辑器，这里安装的是TexStudio）sudo apt-get install texstudio 配置 设置编译器为XeLaTeX，TeXstudio中在Options-\u003eConfigure TeXstudio-\u003eBuild-\u003eDefault Compiler中更改默认编译器为XeLaTeX 更改软件界面语言，将Options-\u003eConfigure TeXstudio-\u003eGeneral-\u003eLanguage改为zh-CN即可将界面设置为中文 其他工具和资源 ","date":"2022-06-30","objectID":"/ubuntu20.04%E5%AE%89%E8%A3%85latex%E5%B9%B6%E9%85%8D%E7%BD%AE%E4%B8%AD%E6%96%87%E7%8E%AF%E5%A2%83/:0:0","tags":["Latex"],"title":"Ubuntu20.04安装LaTeX并配置中文环境","uri":"/ubuntu20.04%E5%AE%89%E8%A3%85latex%E5%B9%B6%E9%85%8D%E7%BD%AE%E4%B8%AD%E6%96%87%E7%8E%AF%E5%A2%83/"},{"categories":["软件安装"],"content":"Mathpix 在写论文或者文档的时候，经常会碰到要输入数学公式的情况。如果去手敲，效率很低。这里介绍一个数学公式的latex语句生成工具 Mathpix。 碰到文献里的数学公式，只需要用这个软件，就可以轻松获得这个公式的latex语句。 ","date":"2022-06-30","objectID":"/ubuntu20.04%E5%AE%89%E8%A3%85latex%E5%B9%B6%E9%85%8D%E7%BD%AE%E4%B8%AD%E6%96%87%E7%8E%AF%E5%A2%83/:0:1","tags":["Latex"],"title":"Ubuntu20.04安装LaTeX并配置中文环境","uri":"/ubuntu20.04%E5%AE%89%E8%A3%85latex%E5%B9%B6%E9%85%8D%E7%BD%AE%E4%B8%AD%E6%96%87%E7%8E%AF%E5%A2%83/"},{"categories":["软件安装"],"content":"模板 模板这里分享一个latex模板网站 模板 ","date":"2022-06-30","objectID":"/ubuntu20.04%E5%AE%89%E8%A3%85latex%E5%B9%B6%E9%85%8D%E7%BD%AE%E4%B8%AD%E6%96%87%E7%8E%AF%E5%A2%83/:0:2","tags":["Latex"],"title":"Ubuntu20.04安装LaTeX并配置中文环境","uri":"/ubuntu20.04%E5%AE%89%E8%A3%85latex%E5%B9%B6%E9%85%8D%E7%BD%AE%E4%B8%AD%E6%96%87%E7%8E%AF%E5%A2%83/"},{"categories":["软件安装"],"content":"APT软件包管理系统 apt-get install apt一般直接安装已经编译好的可执行文件，会直接帮你处理依赖关系，apt-get install安装目录是包的维护者确定的，不是用户。系统安装软件一般在/usr/share，可执行的文件在/usr/bin，配置文件可能安装到了/etc下等。 apt-get remove 卸载已安装的软件包（保留配置文件） apt-get remove -purge 卸载已安装的软件包同时删除配置文件 apt-get autoremove 删除为了满足其他软件包的依赖而安装的，但现在不再需要的软件包(新手注意在桌面版的Ubuntu系统下尽量不要使用) apt-get update 更新软件信息数据库 apt-get upgrade 进行系统升级，即更新已安装的包 apt-get autoclean如果你的硬盘空间不大的话，可以定期运行这个程序，将已经删除了的软件包的.deb安装文件从硬盘中删除掉。如果你仍然需要硬盘空间的话，可以试试apt-get clean，这会把你已安装的软件包的安装包也删除掉，当然多数情况下这些包没什么用了，因此这是个为硬盘腾地方的好办法。 apt-get clean 类似上面的命令，但它删除包缓存中的所有包。这是个很好的做法，因为多数情况下这些包没有用了。但如果你是拨号上网的话，就得重新考虑了。 源码 这种软件包里面都是源程序，没有编译过，经过编译后才能安装。源码安装大致可以分为三步:（./configure）–＞ 编译（sudo make） –＞ 安装（sudo make install） 配置：这是编译源代码的第一步，通过 ./configure 命令完成。执行此步以便为编译源代码作准备。常用的选项有 --prefix=PREFIX，用以指定程序的安装位置。更多的选项可通过 --help 查询。也有某些程序无需执行此步。如果配置了–prefix，如：./configure --prefix=/usr/local/test安装后的所有资源文件都会被放在/usr/local/test目录中，不会分散到其他目录。 编译：一旦配置通过，可即刻使用 make 指令来执行源代码的编译过程。视软件的具体情况而定，编译所需的时间也各有差异，我们所要做的就是耐心等候和静观其变。此步虽然仅下简单的指令，但有时候所遇到的问题却十分复杂。较常碰到的情形是程序编译到中途却无法圆满结束。此时，需要根据出错提示分析以便找到应对之策。 安装：如果编译没有问题，那么执行 sudo make install 就可以将程序安装到系统中了。 当某个安装的软件不再需要时，只须简单的删除该安装目录，就可以把软件卸载干净；移植软件只需拷贝整个目录到另外一个机器即可（相同的操作系统下）当然要卸载程序，也可以在原来的make目录下用一次make uninstall，但前提是Makefile文件有uninstall命令。 如果没有配置--prefix选项，源码包也没有提供make uninstall，则可以通过以下方式可以完整卸载：设置一个临时目录重新安装一遍，如： ./configure --prefix=/tmp/to_remove \u0026\u0026makeinstall然后遍历/tmp/to_remove的文件，删除对应安装位置的文件即可（因为/tmp/to_remove里的目录结构就是没有配置--prefix选项时的目录结构） locate 找到要卸载的软件的安装位置，然后删除所有对应的文件。例如： sudo rm -r /usr/local/include/vtk-8.2 /usr/local/lib/cmake/vtk-8.2/ /usr/local/share/doc/vtk-8.2 DPKG Ubuntu系统中，软件通常以.deb格式的包文件发布，它是一种预编译软件包。deb包中除了包含已编译的软件，通常还包括软件的拷贝路径、对其他软件包的依赖关系记录、一个比较通用的配置文件以及软件的描述、版本、作者、类别、占用空间等信息。针对dpkg来说，它只能安装一个.deb格式的包，同时会通知你安装这个包需要什么样的依赖，但是不会安装那些依赖文件，同时也对这个包进行配置，因为那些依赖包并没有下载安装。 deb软件包命令遵行如下约定： **soft_ver_rev_arch.deb**soft为软件包名，ver为版本号，rev为Ubuntu修订版本号，arch为目标架构名称 dpkg -i | --install xxx.deb 安装deb安装包 dpkg -r | --remove xxx.deb 删除安装包 dpkg -r -p |--purge xxx.deb 连同配置文件一起删除 dpkg -I | -info xxx.deb 产看软件包信息 dpkg -L xxx.deb 查看文件拷贝信息 dpkg -l 查看系统中以安装软件包信息 dpkg-reconfigure xxx 重新配置软件包 有些时候，使用dpkg安装一个软件包，系统会提示该软件包依赖其他软件包。这时，需要先安装其他软件包，知道满足依赖关系为止。或者同时安装多个软件包，如：dpkg -i aaa.deb bbb.deb ccc.deb 。但是，如果一个软件依赖关系过于复杂，使用\"dpkg\"来安装它，并不是一个明智的选择，这个时候需要用到APT软件包管理系统。APT可以自动的检查依赖关系，通过预设的方法来获得相关软件包，并自动安装配置它。事实上，在多数情况下，我们推荐使用APT软件包管理系统。 ","date":"2022-06-30","objectID":"/ubuntu%E5%AE%89%E8%A3%85%E5%8D%B8%E8%BD%BD%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/:0:0","tags":["Linux"],"title":"Ubuntu安装卸载基础知识","uri":"/ubuntu%E5%AE%89%E8%A3%85%E5%8D%B8%E8%BD%BD%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}]